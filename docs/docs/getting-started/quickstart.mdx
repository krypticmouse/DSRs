---
title: "Quickstart"
description: "Start building awesome LLM pipelines in minutes"
icon: "rocket"
---

We'll begin with a 2-minute quickstart to get you running immediately. Afterward, a more detailed, chapter-by-chapter guided tour will deconstruct the concepts, empowering you to build truly sophisticated AI pipelines.

## Quickstart: Your First Prediction in 2 Minutes
This section is designed for action. Follow these steps to get from a new project to a working LLM-powered program as fast as possible.
**Prerequisites:**
*   A working Rust toolchain.
*   An [OpenAI API key](https://platform.openai.com/api-keys).
### Step 1: Create a New Project
```bash
cargo new dsrs_quickstart
cd dsrs_quickstart
```
### Step 2: Add Dependencies
We'll add `dspy-rs` (with the recommended `dsrs` alias) and a few essential support crates.
```bash
cargo add dsrs --package dspy-rs
cargo add tokio -F full
cargo add anyhow
cargo add secrecy
```
### Step 3: Set Your API Key
Create a `.env` file in the root of your `dsrs_quickstart` project and add your API key.
```dotenv
# .env
OPENAI_API_KEY="sk-..."
```
### Step 4: Write the Code
Replace the contents of `src/main.rs` with the following complete example:
```rust
// src/main.rs
use dsrs::prelude::*;
use anyhow::Result;
use secrecy::SecretString;
// 1. Define a "Signature"
// This is a typed contract for what the LLM should do.
// The doc comment becomes the main instruction.
#[Signature]
/// Answer questions about geography.
struct GeographyQA {
    #[input]
    question: String,
    #[output]
    answer: String,
}
#[tokio::main]
async fn main() -> Result<()> {
    // 2. Configure the Language Model (one-time setup)
    let api_key = std::env::var("OPENAI_API_KEY")
        .expect("OPENAI_API_KEY must be set in your .env file");
    configure(
        LM::builder()
            .api_key(SecretString::from(api_key))
            .build(),
        ChatAdapter::default(),
    );
    // 3. Create a "Predictor" module for our signature
    let qa_predictor = Predict::new(GeographyQA::new());
    // 4. Prepare an "Example" with our input data
    let user_question = example! {
        "question": "input" => "What is the highest mountain in North America?",
    };
    // 5. Run the prediction
    println!("Asking the LLM...");
    let result = qa_predictor.forward(user_question).await?;
    // 6. Use the strongly-typed "Prediction" output
    println!("\nAnswer: {}", result.get("answer", None));
    Ok(())
}
```
### Step 5: Run It!
Execute the program from your terminal:
```bash
# cargo run
```
You should see output similar to this:
```
Asking the LLM...
Answer: "The highest mountain in North America is Denali."
```
Congratulations! You've successfully executed your first `DSRs` pipeline. You defined a typed task, sent it to an LLM, and got a structured response back.
To understand the magic behind each of these steps, continue to our Guided Tour.

